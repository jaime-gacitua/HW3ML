View(trainData.y)
View(trainData.x)
View(trainData.y)
lasso.mod=glmnet(trainData.x, as.vector(trainData.y), alpha = 1)
lasso.mod=glmnet(trainData.x, unlist(trainData.y), alpha = 1)
plot(lasso.mod)
lasso.mod=cv.glmnet(trainData.x, unlist(trainData.y), alpha = 1)
plot(lasso.mod)
lasso.mod=glmnet(trainData.x, unlist(trainData.y), alpha = 1)
lasso.cv=cv.glmnet(trainData.x, unlist(trainData.y), alpha = 1)
plot(lasso.mod)
plot(lasso.cv)
plot(lasso.cv)
lasso.cv
trainData.y.log <- log(trainData.y)
trainData.y.log <- log(trainData.y)
lasso.cv.log=cv.glmnet(trainData.x, unlist(trainData.y.log), alpha = 1)
plot(lasso.cv.log)
lasso.cv.log
out = glmnet(trainData.x, unlist(trainData.y), alpha=1)
lasso.pred = predict(lasso.mod,s=bestlam)
lasso.pred = predict(out,type = "coefficients", s=bestlam)
bestlam = lasso.cv$lambda.min
out = glmnet(trainData.x, unlist(trainData.y), alpha=1)
lasso.pred = predict(out,type = "coefficients", s=bestlam)
lasso.coef = predict(out,type = "coefficients", s=bestlam)
lasso.coef
trainData.y.log <- log(trainData.y)
lasso.cv.log=cv.glmnet(trainData.x, unlist(trainData.y.log), alpha = 1)
lasso.cv.log
plot(lasso.cv.log)
bestlam.log = lasso.cv.log$lambda.min
out.log = glmnet(trainData.x, unlist(trainData.y.log), alpha=1)
lasso.coef.log = predict(out,type = "coefficients", s=bestlam.log)
lasso.coef.log
names(trainData.x)
names(trainData.x)
names(trainData.x)
View(trainData.x)
lasso.coef.log[lasso.coef.log!=0]
lasso.coef.log
testData.y <- testData$totcomp
means.x <- mean(trainData[!y.var])
means.x <- means(trainData[!y.var])
means.x <- mean(trainData[!y.var])
trainData[!y.var]
means.x <- mean(as.dataFrame(trainData[!y.var]))
means.x <- mean(trainData[!y.var])
means.x <- mean(trainData)
means.x <- mean(trainData)
means.x <- colMeans(trainData)
means.x <- colMeans(trainData[!y.var])
means.x
names(testData.x)
testData.x <- testData
testData.y <- testData$totcomp
testData.x <- testData
means.x <- colMeans(trainData[!y.var])
means.x
testData.x
testData.x <- testData[!y.var]
means.x <- colMeans(trainData[!y.var])
testData.x
means.x
stdev.x <- sd(trainData[!y.var])
stdev.x <- colSd(trainData[!y.var])
stdev.x <- sd(trainData[!y.var])
stdev.x <- sd(as.data.frame(trainData[!y.var]))
stdev.x <- sd(trainData[!y.var])
stdev.x <- sd(trainData[!y.var])
stdev.x <- sd(as.matrix(trainData[!y.var]))
means.x
stdev.xtestData.x
stdev.x
summary(trainData[!y.var])
means.x <- colMeans(trainData[,!y.var])
summary(trainData[,!y.var])
stdev.x <- sd(trainData[,!y.var])
means.x
stdev.x <- sd(trainData)
View(trainData)
stdev.x <- sqrt(colVars(trainData))
stdev.x <- colVars(trainData)
stdev.x <- colVar(trainData)
stdev.x <- colVars(trainData)
require(boa)
install.packages("boa")
require(boa)
stdev.x <- colVars(trainData)
stdev.x <- colVars(trainData)
means.x <- colMeans(trainData[,!y.var])
stdev.x <- colVars(trainData)
stdev.x <- colStdevs(trainData)
stdev.x <- diag(var(trainData))
stdev.x <- diag(var(trainData[,!y.var]))
means.x
stdev.x
stdev.x <- sqrt(diag(var(trainData[,!y.var])))
means.x
stdev.x
means.x
stdev.x
testData.x <- (testData.x - means.x) - stdev.x
testData.x
testData.y <- testData$totcomp
testData.x <- testData[!y.var]
# Get Mean and Stdev of training set to apply to the X's in the test set
means.x <- colMeans(trainData[,!y.var])
stdev.x <- sqrt(diag(var(trainData[,!y.var])))
testData.x <- (testData.x - means.x) / stdev.x
testData.x
means.x <- colMeans(trainData[,!y.var])
stdev.x <- sqrt(diag(var(trainData[,!y.var])))
testData.x <- (testData.x - means.x) / stdev.x
lasso.pred = predict(out, s=bestlam, newx=testData.x)
lasso.pred = predict(out, s=bestlam, newx=testData.x)
lasso.pred = predict(out,s=bestlam,newx=testData.x)
### Step 6 Fitting a Lasso model with 10-fold CV
lasso.cv=cv.glmnet(trainData.x, unlist(trainData.y), alpha = 1)
plot(lasso.cv)
bestlam = lasso.cv$lambda.min
out = glmnet(trainData.x, unlist(trainData.y), alpha=1)
lasso.coef = predict(out,type = "coefficients", s=bestlam)
lasso.coef
### Step 7 Fitting a Lasso model with 10-fold CV to log(totcomp)
trainData.y.log <- log(trainData.y)
lasso.cv.log=cv.glmnet(trainData.x, unlist(trainData.y.log), alpha = 1)
plot(lasso.cv.log)
lasso.cv.log
bestlam.log = lasso.cv.log$lambda.min
out.log = glmnet(trainData.x, unlist(trainData.y.log), alpha=1)
lasso.coef.log = predict(out,type = "coefficients", s=bestlam.log)
lasso.coef.log
### Step 8
testData.y <- testData$totcomp
testData.x <- testData[!y.var]
# Get Mean and Stdev of training set to standarize the X's in the test set
means.x <- colMeans(trainData[,!y.var])
stdev.x <- sqrt(diag(var(trainData[,!y.var])))
testData.x <- (testData.x - means.x) / stdev.x
# Use best lambda to predict test set
lasso.pred = predict(out,s=bestlam,newx=testData.x)
lasso.pred <- predict(out,testData.x,s=bestlam)
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
### Step 5
# Standarizing independent variables
y.var = names(trainData) %in% c('totcomp')
trainData.x <- scale(trainData[!y.var])
trainData.y <- trainData[y.var]
# REMEMBER When I standarize the test set, I have to use the same mean and stdev to center and scale.
### Step 6 Fitting a Lasso model with 10-fold CV
lasso.cv=cv.glmnet(trainData.x, unlist(trainData.y), alpha = 1)
plot(lasso.cv)
bestlam = lasso.cv$lambda.min
out = glmnet(trainData.x, unlist(trainData.y), alpha=1)
lasso.coef = predict(out,type = "coefficients", s=bestlam)
lasso.coef
### Step 7 Fitting a Lasso model with 10-fold CV to log(totcomp)
trainData.y.log <- log(trainData.y)
lasso.cv.log=cv.glmnet(trainData.x, unlist(trainData.y.log), alpha = 1)
plot(lasso.cv.log)
lasso.cv.log
bestlam.log = lasso.cv.log$lambda.min
out.log = glmnet(trainData.x, unlist(trainData.y.log), alpha=1)
lasso.coef.log = predict(out,type = "coefficients", s=bestlam.log)
lasso.coef.log
### Step 8
testData.y <- testData$totcomp
testData.x <- testData[!y.var]
# Get Mean and Stdev of training set to standarize the X's in the test set
means.x <- colMeans(trainData[,!y.var])
stdev.x <- sqrt(diag(var(trainData[,!y.var])))
testData.x <- (testData.x - means.x) / stdev.x
# Use best lambda to predict test set
lasso.pred <- predict(out,testData.x,s=bestlam)
require(glmnet)
### Step 5
# Standarizing independent variables
y.var = names(trainData) %in% c('totcomp')
trainData.x <- scale(trainData[!y.var])
trainData.y <- trainData[y.var]
# REMEMBER When I standarize the test set, I have to use the same mean and stdev to center and scale.
### Step 6 Fitting a Lasso model with 10-fold CV
lasso.cv=cv.glmnet(trainData.x, unlist(trainData.y), alpha = 1)
plot(lasso.cv)
bestlam = lasso.cv$lambda.min
out = glmnet(trainData.x, unlist(trainData.y), alpha=1)
lasso.coef = predict(out,type = "coefficients", s=bestlam)
lasso.coef
### Step 7 Fitting a Lasso model with 10-fold CV to log(totcomp)
trainData.y.log <- log(trainData.y)
lasso.cv.log=cv.glmnet(trainData.x, unlist(trainData.y.log), alpha = 1)
plot(lasso.cv.log)
lasso.cv.log
bestlam.log = lasso.cv.log$lambda.min
out.log = glmnet(trainData.x, unlist(trainData.y.log), alpha=1)
lasso.coef.log = predict(out,type = "coefficients", s=bestlam.log)
lasso.coef.log
### Step 8
testData.y <- testData$totcomp
testData.x <- testData[!y.var]
# Get Mean and Stdev of training set to standarize the X's in the test set
means.x <- colMeans(trainData[,!y.var])
stdev.x <- sqrt(diag(var(trainData[,!y.var])))
testData.x <- (testData.x - means.x) / stdev.x
# Use best lambda to predict test set
lasso.pred <- predict(out,testData.x,s=bestlam)
lasso.pred <- predict(out,testData.x,s=bestlam)
lasso.pred <- predict(out,newx=testData.x,s=bestlam)
bestlam
testData.x
testData.x <- as.matrix((testData.x - means.x) / stdev.x)
testData.x
lasso.pred <- predict(out,newx=testData.x,s=bestlam)
lasso.error <- mean((lasso.pred-testData.y)^2)
lasso.error
lasso.pred.log <- predict(out.log,newx=testData.x,s=bestlam)
lasso.error.log <- mean((lasso.pred-log(testData.y))^2)
lasso.error.log
lasso.pred.log <- predict(out.log,newx=testData.x,s=bestlam.log)
lasso.error.log <- mean((lasso.pred-log(testData.y))^2)
lasso.error.log
bestlam.log
lasso.pred.log <- predict(out.log,newx=trainData.x,s=bestlam.log)
lasso.error.log <- mean((lasso.pred-log(trainData.y))^2)
lasso.error.log
lasso.pred.log <- predict(out.log,newx=trainData.x,s=bestlam.log)
lasso.error.log <- mean((lasso.pred.log-log(trainData.y))^2)
lasso.error.log
## Second Lasso Model (log)
lasso.pred.log <- predict(out.log,newx=testData.x,s=bestlam.log)
lasso.error.log <- mean((lasso.pred.log-log(testData.y))^2)
lasso.error.log
lasso.pred.log
c(lasso.pred.log, log(testData.y)
c(lasso.pred.log, log(testData.y))
c(lasso.pred.log, log(testData.y))
?c
c(lasso.pred.log, log(testData.y), ncol=2)
c(lasso.pred.log, log(testData.y), nrow= length(lasso.pred.log), ncol=2)
matrix(c(lasso.pred.log, log(testData.y), nrow=length(lasso.pred.log)), ncol=2)
len(lasso.pred.log)
length(lasso.pred.log)
length(log(testData.y))
matrix(c(lasso.pred.log, log(testData.y)), nrow=length(lasso.pred.log))
lasso.error.log <- mean((exp(lasso.pred.log)-testData.y)^2)
lasso.pred.log <- predict(out.log,newx=testData.x,s=bestlam.log)
lasso.error.log <- mean((exp(lasso.pred.log)-testData.y)^2)
lasso.error.log
lasso.pred.log <- predict(out.log,newx=testData.x,s=bestlam.log)
lasso.error.log <- mean((exp(lasso.pred.log)-testData.y)^2)
lasso.error.log
length(lasso.pred.log)
length(log(testData.y))
matrix(c(lasso.pred.log, log(testData.y)), nrow=length(lasso.pred.log))
matrix(c(lasso.pred.log, log(testData.y), lasso.pred.log - log(testData.y) ), nrow=length(lasso.pred.log))
matrix(c(exp(lasso.pred.log), testData.y, exp(lasso.pred.log) - log(testData.y) ), nrow=length(lasso.pred.log))
matrix(c(lasso.pred.log, exp(lasso.pred.log), testData.y, exp(lasso.pred.log) - log(testData.y) ), nrow=length(lasso.pred.log))
lasso.pred <- predict(out,newx=testData.x,s=bestlam)
lasso.error <- mean((lasso.pred-testData.y)^2)
lasso.error
lasso.pred.log <- predict(out.log,newx=testData.x,s=bestlam.log)
lasso.error.log <- mean((exp(lasso.pred.log)-testData.y)^2)
lasso.error.log
means(trainData.x)
mean(trainData.x)
colMeans(trainData.x)
colStdev(trainData.x)
sd(trainData.x$salary)
trainData.x <- to.dataFrame(scale(trainData[!y.var]))
View(trainData.x)
sd(trainData.x[,1])
sd(trainData.x[,2])
lasso.coef.log
means.x <- colMeans(trainData[,!y.var])
summary(trainData)
var(trainData[,1])
stdev.x
var(trainData[,1])
var(trainData[,3])
testData.x
lasso.pred.log <- predict(out.log,newx=testData.x,s=bestlam.log)
lasso.error.log <- mean((exp(lasso.pred.log)-testData.y)^2)
lasso.error.log
matrix(c(lasso.pred.log, exp(lasso.pred.log), testData.y, exp(lasso.pred.log) - log(testData.y) ), nrow=length(lasso.pred.log))
plot(lasso.cv.log)
lasso.cv.log
matrix(c(lasso.pred.log, exp(lasso.pred.log), testData.y, exp(lasso.pred.log) - log(testData.y) ), nrow=length(lasso.pred.log))
lasso.error.log <- mean((exp(lasso.pred.log)-testData.y)^2)
lasso.error.log
bestlam
bestlam.log
Default
names(default)
names(Default)
require(ISLR)
summary(Default)
names(Default)
set.seed(1)
glm.fit=glm(default~student+balance+income,data=Default,family=binomial)
summary(glm.fit)
set.seed(1)
glm.fit=glm(default~balance+income,data=Default,family=binomial)
summary(glm.fit)
glm.fit$coefficients
glm.fit$coefficients[1]
glm.fit$coefficients[2]
boot.fn <- function(def.data, obs.index) {
glm.fit = glm(default~balance+income, data=def.data[obs.index], family=binomial)
coefs = c(glm.fit$coefficients[2], glm.fit$coefficients[3])
return(coefs)
}
boot.fn(Default, c(1,2,3,4,5,6))
Default[2]
Default[,2]
boot.fn(Default, c(1,2,3,4,5,6))
## Part b
data2 = subset(def.data, obs.index)
boot.fn <- function(def.data, obs.index) {
glm.fit = glm(default~balance+income, data=data2, family=binomial)
coefs = c(glm.fit$coefficients[2], glm.fit$coefficients[3])
return(coefs)
}
boot.fn(Default, c(1,2,3,4,5,6))
subset(Default, )
Default[,c(1,2,3,4)]
Default[c(1,2,3,4),]
## Part b
boot.fn <- function(def.data, obs.index) {
data2 = def.data[obs.index,]
glm.fit = glm(default~balance+income, data=data2, family=binomial)
coefs = c(glm.fit$coefficients[2], glm.fit$coefficients[3])
return(coefs)
}
boot.fn(Default, c(1,2,3,4,5,6))
boot.fn(Default, c(1,2,3,4,5,6,7))
boot.fn(Default, c(1,2,3,4,5,6,7,8,9,10))
?boot
require(boot)
?boot
names(city)
city
ratio <- function(d, w) sum(d$x * w)/sum(d$u * w)
boot(city, ratio, R = 999, stype = "w")
boot(Default, boot.fn, R = 999)
require(MASS)
require(boot)
## checking there are no NA values
## Part a
Boston$medv[is.na(Boston$medv)]
medv.mean <- mean(Boston$medv)
## Part b
medv.se <- sd(Boston$medv) / sqrt(length(Boston$medv))
## Part c
se.function = function(data,index){     # INput args are: (i) the data and (ii) an index vector defining what observations to use
data2 = data[index]
return(mean(data2))
}
boot(Boston$medv, se.function, R=1000)
boot(Boston$medv, se.function, R=10000)
boot(Boston$medv, se.function, R=100000)
se.function(Boston$medv, sample(length(Boston$medv), 50))
se.function(Boston$medv, sample(length(Boston$medv), 500))
se.boot <- boot(Boston$medv, se.function, R=100000)
se.boot$t0[1]
se.boot$t0[1]+ 2*se.boot$t0[3]
se.boot$t0[1]+ 2*se.boot$t0[2]
se.boot$t0
se.boot$t
se.boot$statistic()
se.boot$strata
se.boot$t0[1]+ 2*sd(se.boot$t)
se.boot$t0[1] + 2*sd(se.boot$t)
se.boot$t0[1] - 2*sd(se.boot$t)
sd(se.boot$t)
se.boot <- boot(Boston$medv, se.function, R=100000)
se.boot
## Part d
se.boot$t0[1] + 2*sd(se.boot$t)
se.boot$t0[1] - 2*sd(se.boot$t)
t.test(Boston$medv)
summary(Boston$medv)
summary(Boston$medv)[3]
med.function = function(data,index){     # INput args are: (i) the data and (ii) an index vector defining what observations to use
data2 = data[index]
return(summary(data2)[3])
}
med.boot <- boot(Boston$medv, med.function, R=100000)
med.boot
?quantile
med.percentile10 <- quantile(Boston$medv, 0.5)
med.percentile10
med.percentile10 <- quantile(Boston$medv, 0.1)
med.percentile10
perc10.function = function(data,index){     # INput args are: (i) the data and (ii) an index vector defining what observations to use
data2 = data[index]
return(quantile(data2, 0.1))
}
med.boot.perc10 <- boot(Boston$medv, perc10.function, R=100000)
med.boot.perc10
require(ggplot2)
require(GGally)
require(glmnet)
## Step 1
## Read the data into a data frame
ceo_data <- read.csv(".\\ceo.csv")
ceo_data<-as.data.frame(ceo_data)
names(ceo_data)
summary(ceo_data)
## Step 2
## Remove last column of de data frame
ceo_data <- ceo_data[,0:7]
names(ceo_data)
summary(ceo_data)
## Step 3 and 4
## Selecting sufficiently random training and test sets.
## For now deleted using the if false statement. Takes a lot of time.
#if(FALSE){
minDif <- 999999999999
count <- 0
iter <- 50000
while(count < iter){
#Generate random sample for training and testing
train_index <- sample(nrow(ceo_data), floor(nrow(ceo_data) * 0.75))
trainData <- ceo_data[train_index, ]
testData <- ceo_data[-train_index, ]
difTrain <- abs(cov(ceo_data) - cov(trainData))
difTest <- abs(cov(ceo_data) - cov(testData))
totalDif <- sum(difTrain) + sum(difTest)
if(totalDif < minDif){
train_index_min <- train_index
minDif <- totalDif
}
count <- count + 1
if(count - floor(count/1000)*1000 == 0){
print(count)
}
}
# Use the best set
trainData <- ceo_data[train_index_min, ]
testData <- ceo_data[-train_index_min, ]
#}
### Step 4.
## EDA to check if the split is random enough
## Pair Plots
ggpairs(ceo_data, diag=list(continuous="densityDiag",   discrete="barDiag"), axisLabels="show")
ggpairs(trainData, diag=list(continuous="densityDiag",   discrete="barDiag"), axisLabels="show", aes(colour='red'))
ggpairs(testData, diag=list(continuous="densityDiag",   discrete="barDiag"), axisLabels="show", aes(colour='blue'))
## Box Plots
boxplot(ceo_data$salary, trainData$salary, testData$salary,
names = list("DataSet", "Train", 'Test'), main = "Salary")
boxplot(ceo_data$totcomp, trainData$totcomp, testData$totcomp,
names = list("DataSet", "Train", 'Test'), main = "TotComp")
boxplot(ceo_data$tenure, trainData$tenure, testData$tenure,
names = list("DataSet", "Train", 'Test'), main = "tenure")
boxplot(ceo_data$age, trainData$age, testData$age,
names = list("DataSet", "Train", 'Test'), main = "age")
boxplot(ceo_data$sales, trainData$sales, testData$sales,
names = list("DataSet", "Train", 'Test'), main = "sales")
boxplot(ceo_data$profits, trainData$profits, testData$profits,
names = list("DataSet", "Train", 'Test'), main = "profits")
boxplot(ceo_data$assets, trainData$assets, testData$assets,
names = list("DataSet", "Train", 'Test'), main = "assets")
### Step 5
# Standarizing independent variables
y.var = names(trainData) %in% c('totcomp')
trainData.x <- scale(trainData[!y.var])
trainData.y <- trainData[y.var]
colMeans(trainData.x)
sd(trainData.x[,2])
# REMEMBER When I standarize the test set, I have to use the same mean and stdev to center and scale.
### Step 6 Fitting a Lasso model with 10-fold CV
lasso.cv=cv.glmnet(trainData.x, unlist(trainData.y), alpha = 1)
plot(lasso.cv)
bestlam = lasso.cv$lambda.min
out = glmnet(trainData.x, unlist(trainData.y), alpha=1)
lasso.coef = predict(out,type = "coefficients", s=bestlam)
lasso.coef
### Step 7 Fitting a Lasso model with 10-fold CV to log(totcomp)
trainData.y.log <- log(trainData.y)
lasso.cv.log=cv.glmnet(trainData.x, unlist(trainData.y.log), alpha = 1)
plot(lasso.cv.log)
lasso.cv.log
bestlam.log = lasso.cv.log$lambda.min
out.log = glmnet(trainData.x, unlist(trainData.y.log), alpha=1)
lasso.coef.log = predict(out,type = "coefficients", s=bestlam.log)
lasso.coef.log
### Step 8
testData.y <- testData$totcomp
testData.x <- testData[!y.var]
# Get Mean and Stdev of training set to standarize the X's in the test set
means.x <- colMeans(trainData[,!y.var])
stdev.x <- sqrt(diag(var(trainData[,!y.var])))
testData.x <- as.matrix((testData.x - means.x) / stdev.x)
## First Lasso Model
lasso.pred <- predict(out,newx=testData.x,s=bestlam)
lasso.error <- mean((lasso.pred-testData.y)^2)
lasso.error
## Second Lasso Model (log)
lasso.pred.log <- predict(out.log, newx=testData.x, s=bestlam.log)
lasso.error.log <- mean((exp(lasso.pred.log)-testData.y)^2)
lasso.error.log
matrix(c(lasso.pred.log, exp(lasso.pred.log), testData.y, exp(lasso.pred.log) - log(testData.y) ), nrow=length(lasso.pred.log))
lasso.error.log <- mean((lasso.pred.log)-log(testData.y)^2)
lasso.error.log
lasso.error.log <- mean(((lasso.pred.log)-log(testData.y))^2)
lasso.error.log
?cv.glmnet
require(ggplot2)
require(GGally)
require(glmnet)
?cv.glmnet
